{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prototype Chatbot with Sentiments - BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can access the combination of the Chatbot we downloaded from Huggingface and the BERT Model that analyzes Sentiment: https://colab.research.google.com/drive/1UVocScWBv3zDgruO6nvds5m0RObqJNDd?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "169eed8df19645b9b92230538b4701c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Student\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Student\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde87358c3a34f798a1bc7e77d952d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/964k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecbd05afd9b149229231dce655f7c0c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/345k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f8bdcb66dd4837b757c05fb7a2325f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/205 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3339f1832c024b629527389b3c019329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ddae5c394949b7a7662ba956b28f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.51k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b987cd4f821f4156bb43085afe422215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/350M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a5fb144e7a4488982fe8eae5d99718b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/311 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: Positive\n",
      "Negative Sentiment Probability: 0.4313\n",
      "Positive Sentiment Probability: 0.5687\n",
      "Chatbot: that's great! i'm happy for you. what's going on in your life?\n",
      "Exiting the program.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BlenderbotSmallForConditionalGeneration, BlenderbotSmallTokenizer\n",
    "\n",
    "# Load DistilBERT for sentiment analysis\n",
    "sentiment_model_name = \"distilbert-base-uncased\"\n",
    "sentiment_tokenizer = AutoTokenizer.from_pretrained(sentiment_model_name)\n",
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(sentiment_model_name)\n",
    "\n",
    "# Load BlenderBot chatbot\n",
    "chatbot_model_name = \"facebook/blenderbot_small-90M\"\n",
    "chatbot_tokenizer = BlenderbotSmallTokenizer.from_pretrained(chatbot_model_name)\n",
    "chatbot_model = BlenderbotSmallForConditionalGeneration.from_pretrained(chatbot_model_name)\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"Enter your text (or 'exit' to quit): \")\n",
    "\n",
    "    if user_input.lower() == 'exit':\n",
    "        print(\"Exiting the program.\")\n",
    "        break\n",
    "\n",
    "    # Sentiment analysis using DistilBERT\n",
    "    user_input_tokens = sentiment_tokenizer(user_input, padding=True, truncation=True, return_tensors='pt')\n",
    "    user_input_encoded = {key: val for key, val in user_input_tokens.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        user_output = sentiment_model(**user_input_encoded)\n",
    "        user_logits = user_output.logits\n",
    "        user_probs = torch.softmax(user_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    class_names = {0: \"Negative\", 1: \"Positive\"}\n",
    "    predicted_sentiment_idx = np.argmax(user_probs)\n",
    "    predicted_sentiment = class_names[predicted_sentiment_idx].capitalize()\n",
    "\n",
    "    # Generate chatbot response based on sentiment\n",
    "    chatbot_input = f\"Predicted Sentiment: {predicted_sentiment}. {user_input}\"\n",
    "    chatbot_input_ids = chatbot_tokenizer.encode(chatbot_input, return_tensors=\"pt\")\n",
    "\n",
    "    chatbot_response_ids = chatbot_model.generate(chatbot_input_ids, max_length=50, num_return_sequences=1, pad_token_id=chatbot_tokenizer.eos_token_id)\n",
    "    chatbot_response = chatbot_tokenizer.decode(chatbot_response_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Print the predicted sentiment, sentiment probabilities, and chatbot response\n",
    "    print(f\"Predicted Sentiment: {predicted_sentiment}\")\n",
    "    for class_idx, class_name in class_names.items():\n",
    "        print(f\"{class_name.capitalize()} Sentiment Probability: {user_probs[0][class_idx]:.4f}\")\n",
    "    print(f\"Chatbot: {chatbot_response}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
